{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reproducing Table 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94Cab7P6Vnk6",
        "colab_type": "text"
      },
      "source": [
        "# **Reproducing Table 2**\n",
        "\n",
        "After reproducing Figure 1 of the paper, the Kaplan-Meier survival curves, we began on reproducing the actual outcome of the paper: Table 2. Table 2 gives the model performance using C-index on 20 studied cancer types, using different combinations of data modalities.\n",
        "\n",
        "Next to the already used clinical data, we had to work with gene expression data (mRNA), miRNA data and whole slide images.\n",
        "\n",
        "We wanted to start with the most simple combination of the data modalities and extend the code later to also use other modalities as well. We began with the second column of the table using miRNA and clinical data. \n",
        "\n",
        "We first wanted to use the author's code for this, to see what outcome we would get. So we tried using the chart1.py code of the authors. \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ssh3VpSXJp1",
        "colab_type": "text"
      },
      "source": [
        "**Mounting Google Drive and importing modules**\n",
        "\n",
        "Because this notebook is made in Google Colab, the necessary modules are placed in a Google Drive, which had to be mounted to use it in Google Colab. \n",
        "That is wat you will see in the next lines of code.\n",
        "The first reproducability problem already started here as their module fetch.py needs the file: fetch_datachache.npz to work. However, this file was nowhere to be found, so we decided to comment out the modules that needed this file, e.g. generators and data, and we tried writing the code for this ourselves. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAf2NREDTaX2",
        "colab_type": "code",
        "outputId": "7b934d77-35a1-462c-e175-738c74134d3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "source": [
        "## Mount drive and import all necessary modules\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os, sys, random, yaml, gc\n",
        "from itertools import *\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import csv\n",
        "import gzip\n",
        "import pickle\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "\n",
        "!pip install lifelines\n",
        "!pip install xmltodict\n",
        "!pip install pandas\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "from lifelines.utils import concordance_index\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "\n",
        "%cd 'drive/My Drive/Reproducibility_Project/MultimodalPrognosis-master/'\n",
        "\n",
        "from utils import *\n",
        "from models import TrainableModel\n",
        "from modules import Highway\n",
        "## Following two modules of authors were not usable, so had to be commented out and written by ourselves\n",
        "# from generators import AbstractPatientGenerator\n",
        "# from data import fetch\n",
        "\n",
        "sys.path.append('/content/drive/My Drive/Reproducibility_Project/MultimodalPrognosis-master/')\n",
        "\n",
        "## Following two modules were written by ourselves and can be found in Github\n",
        "from one_hot_encoding_clinical_dataset import *\n",
        "\n",
        "import IPython"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: lifelines in /usr/local/lib/python3.6/dist-packages (0.24.3)\n",
            "Requirement already satisfied: autograd-gamma>=0.3 in /usr/local/lib/python3.6/dist-packages (from lifelines) (0.4.2)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from lifelines) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from lifelines) (1.18.2)\n",
            "Requirement already satisfied: autograd>=1.3 in /usr/local/lib/python3.6/dist-packages (from lifelines) (1.3)\n",
            "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.6/dist-packages (from lifelines) (3.2.1)\n",
            "Requirement already satisfied: pandas>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from lifelines) (1.0.3)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.6/dist-packages (from autograd>=1.3->lifelines) (0.16.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0->lifelines) (2.4.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0->lifelines) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0->lifelines) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0->lifelines) (1.2.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23.0->lifelines) (2018.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib>=3.0->lifelines) (1.12.0)\n",
            "Requirement already satisfied: xmltodict in /usr/local/lib/python3.6/dist-packages (0.12.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.0.3)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.18.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n",
            "[Errno 2] No such file or directory: 'drive/My Drive/Reproducibility_Project/MultimodalPrognosis-master/'\n",
            "/content/drive/My Drive/Reproducibility_Project/MultimodalPrognosis-master\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0h6ArcnaH0K",
        "colab_type": "text"
      },
      "source": [
        "**Trainable Model**\n",
        "\n",
        "In the following code cell, the trainable model is defined for miRNA and clinical data. The different layers are implemented as presented in the original paper. The code is the original code, written by the authors of the paper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOF28LUicz53",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#For miRNA and clinical\n",
        "\n",
        "MULTIMODAL = sys.argv[1]\n",
        "OUTFILE = f\"results/{MULTIMODAL}_mirna_clinical.txt\"\n",
        "print (OUTFILE)\n",
        "\n",
        "class Network(TrainableModel):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Network, self).__init__()\n",
        "\n",
        "        self.fcm = nn.Linear(1881, 256)\n",
        "        self.fcc = nn.Linear(7, 256)\n",
        "        self.fcg = nn.Linear(60483, 256)\n",
        "        self.highway = Highway(256, 10, f=F.relu)\n",
        "        self.fc2 = nn.Linear(256, 2)\n",
        "        self.fcd = nn.Linear(256, 1)\n",
        "        self.bn1 = nn.BatchNorm1d(256)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        self.bn3 = nn.BatchNorm1d(1, affine=True)\n",
        "\n",
        "    def forward(self, data, mask):\n",
        "\n",
        "        x = data['mirna']\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = F.dropout(x, 0.4)\n",
        "        x = F.tanh(self.fcm(x))\n",
        "\n",
        "        y = data['clinical']\n",
        "        y = y.view(y.shape[0], -1)\n",
        "        y = F.tanh(self.fcc(y))\n",
        "\n",
        "        mean = masked_mean((x, y), (mask[\"mirna\"], mask[\"clinical\"]))\n",
        "\n",
        "        var = masked_variance((x, y), (mask[\"mirna\"], mask[\"clinical\"])).mean()\n",
        "        var2 = masked_mean (((x - mean.mean())**2, (y - mean.mean())**2), \\\n",
        "                            (mask[\"mirna\"], mask[\"clinical\"]))\n",
        "\n",
        "        ratios = var/var2.mean(dim=1)\n",
        "        ratio = ratios.clamp(min=0.02, max=1.0).mean()\n",
        "\n",
        "        x = mean\n",
        "\n",
        "        x = self.bn1(x)\n",
        "        x = F.dropout(x, 0.5, training=self.training)\n",
        "        x = self.highway(x)\n",
        "        x = self.bn2(x)\n",
        "\n",
        "        score = F.log_softmax(self.fc2(x), dim=1)\n",
        "        hazard = self.fcd(x)\n",
        "\n",
        "        return {\"score\": score, \"hazard\": hazard, \"ratio\": ratio.unsqueeze(0)}\n",
        "\n",
        "    def loss(self, pred, target):\n",
        "\n",
        "        vital_status = target[\"vital_status\"]\n",
        "        days_to_death = target[\"days_to_death\"]\n",
        "        hazard = pred[\"hazard\"].squeeze()\n",
        "\n",
        "        loss = F.nll_loss(pred[\"score\"], vital_status)\n",
        "\n",
        "        _, idx = torch.sort(days_to_death)\n",
        "        hazard_probs = F.softmax(hazard[idx].squeeze()[1-vital_status.byte()])\n",
        "        hazard_cum = torch.stack([torch.tensor(0.0)] + list(accumulate(hazard_probs)))\n",
        "        N = hazard_probs.shape[0]\n",
        "        weights_cum = torch.range(1, N)\n",
        "        p, q = hazard_cum[1:], 1-hazard_cum[:-1]\n",
        "        w1, w2 = weights_cum, N - weights_cum\n",
        "\n",
        "        probs = torch.stack([p, q], dim=1)\n",
        "        logits = torch.log(probs)\n",
        "        ll1 = (F.nll_loss(logits, torch.zeros(N).long(), reduce=False) * w1)/N\n",
        "        ll2 = (F.nll_loss(logits, torch.ones(N).long(), reduce=False) * w2)/N\n",
        "        loss2 = torch.mean(ll1 + ll2)\n",
        "\n",
        "        loss3 = pred[\"ratio\"].mean()\n",
        "        \n",
        "        return loss + loss2 + loss3*0.3\n",
        "\n",
        "    def score(self, pred, target):\n",
        "        #R, p = pearsonr(pred, target)\n",
        "        vital_status = target[\"vital_status\"]\n",
        "        days_to_death = target[\"days_to_death\"]\n",
        "        score_pred = pred[\"score\"][:, 1]\n",
        "        hazard = pred[\"hazard\"][:, 0]\n",
        "\n",
        "        auc = roc_auc_score(vital_status, score_pred)\n",
        "        cscore = concordance_index(days_to_death, -hazard, np.logical_not(vital_status))\n",
        "\n",
        "        return {\"AUC\": auc, \"C-index\": cscore, \"Ratio\": pred[\"ratio\"].mean()}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFmcuzcdaxY9",
        "colab_type": "text"
      },
      "source": [
        "**Generating data**\n",
        "\n",
        "As previously mentioned the data generator did not work because of the missing file. Commented out you can see the code as it was originally written by the authors. \n",
        "\n",
        "We tried writing our own code to load the data and process it in such a way that it will eventually be a tensor of clinical data, miRNA data, vital status and days to death. The same format as we thought their data looked like before training. \n",
        "\n",
        "It was really hard figuring out how everything was put together, because there are a lot of different files with different modules the authors originally used.\n",
        "\n",
        "Another problem was working with the data, because it is so large. In the paper there wasn't a specific description of what the data looked like and in what format they used it for training. So we had to gather that information out of all the unstructured codes.\n",
        "For the clinical data we eventually ended up with using one-hot-encoding for race and cancer type. We wrote this code ourselves, imported above as one_hot_encoding_clinical_dataset. The code can be found in Github. In this code the cancer types with abbreviations READ & COAD were given the same one-hot-encoding, as they are on the same line in Table 2 of the paper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9W93YrBaw4Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## The following code of the authors could not been used, so we had to write it ourselves\n",
        "\n",
        "# class DataGenerator(AbstractPatientGenerator):\n",
        "\n",
        "#     def __init_(self, **kwargs):\n",
        "#         super(DataGenerator, self).__init__(**kwargs)\n",
        "\n",
        "#     def sample(self, case, mode='train'):\n",
        "\n",
        "#         mirna_data = fetch.mirna_data(case)\n",
        "\n",
        "#         if mirna_data is not None:\n",
        "#             mirna_data = torch.tensor(mirna_data).float()\n",
        "\n",
        "#         gene_data = fetch.gene_data(case)\n",
        "\n",
        "#         if gene_data is not None:\n",
        "#             gene_data = torch.tensor(gene_data).float()\n",
        "\n",
        "#         clinical_data = fetch.clinical_data_expanded(case)\n",
        "        \n",
        "#         if clinical_data is not None:\n",
        "#             clinical_data = torch.tensor(clinical_data).float()\n",
        "\n",
        "#         vital_status = fetch.vital_status(case)\n",
        "#         days_to_death = fetch.days_to_death(case)\n",
        "#         if days_to_death is False or days_to_death is None:\n",
        "#             vital_status = True\n",
        "#             days_to_death = 20000\n",
        "        \n",
        "#         if MULTIMODAL == 'multi':\n",
        "#             if mode == 'train' and random.randint(1, 4) == 1: clinical_data = None\n",
        "#             if mode == 'train' and random.randint(1, 4) == 1: mirna_data = None\n",
        "#             if mode == 'train' and random.randint(1, 4) == 1: gene_data = None\n",
        "\n",
        "#         if clinical_data is None and mirna_data is None: return None\n",
        "#         if vital_status is None: return None\n",
        "\n",
        "#         return {\"clinical\": clinical_data, \"mirna\": mirna_data},\\\n",
        "#                 {\"vital_status\": torch.tensor(vital_status).long(),\n",
        "#                 \"days_to_death\": torch.tensor(days_to_death).float()}\n",
        "    \n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    model = Network()\n",
        "    model.compile(optim.Adam, lr=8e-4)\n",
        "\n",
        "#---------\n",
        "# Own code to generate data\n",
        "\n",
        "# Clinical data\n",
        "    cancer_data = pd.read_csv('data/pancancer_biospecimen (1).csv', sep='\\t', index_col=False) \n",
        "    cases = []\n",
        "    cancer_types = []\n",
        "    for row in cancer_data['barcode']:\n",
        "        case = row.split('-')[0:3]\n",
        "        case = \"-\".join(case)\n",
        "        cases.append(case)\n",
        "    for row in cancer_data['project']:\n",
        "        cancer_type = row.split('-')[1]\n",
        "        cancer_types.append(cancer_type)\n",
        "\n",
        "    with open('data/original_clinical_dataset.json') as f:\n",
        "        clinical = json.load(f)\n",
        "\n",
        "    clinical_data = np.empty((len(clinical),43),).astype(float)\n",
        "    ID_ref = np.empty(len(clinical)).astype(object)\n",
        "    vital_status = np.empty((len(clinical),1))\n",
        "    days_to_death = np.empty((len(clinical),1))\n",
        "    j=0\n",
        "    for line in clinical:\n",
        "      err = 0\n",
        "      if 'diagnoses' in line:\n",
        "        ID = line['diagnoses'][0]['submitter_id'].split(\"_\")[0]\n",
        "        ID_ref[j] = ID\n",
        "        try: \n",
        "          index = cases.index(ID)\n",
        "        except ValueError: ## Some patients were not present in biospecimen file. This surpresses error message \n",
        "          err = 1\n",
        "\n",
        "        if err == 1:\n",
        "          clinical_data[j] = np.concatenate((np.array([None]),np.array([None]),np.zeros(5),np.zeros(36)),axis=0)\n",
        "        else:\n",
        "          disease = cancer_types[index]\n",
        "          keys = ['gender','age_at_index','race']\n",
        "          if not all(key in line['demographic'] for key in keys): clinical_data[j] = np.concatenate((np.array([None]),np.array([None]),np.zeros(5),np.zeros(36)),axis=0)\n",
        "          clinical_data[j] = np.concatenate((encoding_gender(line['demographic']['gender']),np.array([line['demographic']['age_at_index']]),encoding_race(line['demographic']['race']),encoding_cancer_type(disease)),axis=0)\n",
        "          vital_status[j] = encoding_vital_status(line['demographic']['vital_status'])\n",
        "          if 'days_to_death' in line['demographic']:\n",
        "            days_to_death[j] = line['demographic']['days_to_death']\n",
        "          else:\n",
        "            days_to_death[j] = 20000\n",
        "            \n",
        "      else:\n",
        "        clinical_data[j] = np.concatenate((np.array([None]),np.array([None]),np.zeros(5),np.zeros(36)),axis=0)\n",
        "      j+=1\n",
        "    \n",
        "#MiRNA data\n",
        "    with gzip.open('data/output/miRNA/colorectal_READ_miRNA_data.pkl','rb') as f:\n",
        "        miRNA_out = pickle.load(f)\n",
        "\n",
        "    miRNA_data = np.empty((len(clinical),1881))\n",
        "    for line in list(miRNA_out.index.values):\n",
        "        ID = \"-\".join(line.split('-')[0:3])\n",
        "        indx = np.where(ID_ref == ID)\n",
        "        miRNA_data[indx] = miRNA_out.loc[line] \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOQ7dSojeRMS",
        "colab_type": "text"
      },
      "source": [
        "**Split in train and test data**\n",
        "\n",
        "The paper mentioned that the dataset of 11 160 patients was split into training and test datasets in 85/15 ratio, stratified by cancer type in order to ensure the same distribution of cancers in both the training and the test set. \n",
        "\n",
        "We implemented a code for this ourselves.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-r1kk_jXeP14",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Generate train and test data \n",
        "\n",
        "    #Find train and test indices using cancer_types to guarantee same distribution per cancer type\n",
        "        \n",
        "    cancer_types = np.arange(0,36)\n",
        "    data = clinical_data\n",
        "    test_size = 0.15\n",
        "    training_size = 0.85\n",
        "\n",
        "    train_index = []\n",
        "    test_index = []\n",
        "    for i in cancer_types:\n",
        "      cancer_type = np.zeros((1,data.shape[1]))\n",
        "      cancer_type = np.vstack((cancer_type, data[data[:,7+i] == 1]))\n",
        "      cancer_type = np.delete(cancer_type,0,axis=0)\n",
        "      if len(cancer_type)==0:\n",
        "        pass\n",
        "      else:\n",
        "        rs = ShuffleSplit(1, test_size, training_size)\n",
        "        for train_index_new, test_index_new in rs.split(cancer_type):\n",
        "          train_index = np.append(train_index,train_index_new)\n",
        "          test_index = np.append(test_index,test_index_new)\n",
        "  \n",
        "    trn_idx = train_index\n",
        "    tst_idx = test_index\n",
        "    \n",
        "  #trn_idx, tst_idx = stratify(clinical_data,np.arange(0,36),0.15, 0.85)\n",
        "\n",
        "    # Train data\n",
        "    train = np.empty((len(trn_idx),2)).astype(object)\n",
        "    i=0\n",
        "    for index in trn_idx: \n",
        "        clinical_train = clinical_data[int(index)]\n",
        "        miRNA_train = miRNA_data[int(index)]\n",
        "        vital_status_train = vital_status[int(index)]\n",
        "        days_to_death_train = days_to_death[int(index)]\n",
        "\n",
        "        train[i][0] = {\"clinical\": torch.tensor(clinical_train).float(), \"mirna\": torch.tensor(miRNA_train).float()}\n",
        "        train[i][1] = {\"vital_status\": torch.tensor(vital_status_train).long(), \"days_to_death\": torch.tensor(days_to_death_train).float()}\n",
        "        i+=1\n",
        "\n",
        "    # Test data\n",
        "    test = np.empty((len(tst_idx),2)).astype(object)\n",
        "    j=0\n",
        "    for index in tst_idx: \n",
        "        clinical_test = clinical_data[int(index)]\n",
        "        miRNA_test = miRNA_data[int(index)]\n",
        "        vital_status_test = vital_status[int(index)]\n",
        "        days_to_death_test = days_to_death[int(index)]\n",
        "\n",
        "        test[j][0] = {\"clinical\": torch.tensor(clinical_test).float(), \"mirna\": torch.tensor(miRNA_test).float()}\n",
        "        test[j][1] = {\"vital_status\": torch.tensor(vital_status_test).long(), \"days_to_death\": torch.tensor(days_to_death_test).float()}\n",
        "        j+=1\n",
        "\n",
        "#------\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9Y8Hwm2fNxX",
        "colab_type": "text"
      },
      "source": [
        "**Training the data**\n",
        "\n",
        "In the author's code they eventually end up with for every case an array of dictionairies. But because we took all the cases together, we ended up with a dictionairy of arrays. We thought this wasn't really a problem, because the authors use a stack function to make a dictionary of arrays out of arrays of dictionaries. And as our generated data is already a dictionary of arrays, we decided to remove this function.\n",
        "\n",
        "We were optimistic and thought if having the data in the right format and splitting it in train and test, the code for training the model would do the job. \n",
        "\n",
        "Unfortunately, trying the author's code on our train and test data gave errors instead of results. \n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eEoBOYIfMt4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Author's code\n",
        "\n",
        "    #datagen = DataGenerator(samples=40000, val_samples=10000)\n",
        "\n",
        "    for epochs in range(0, 5):\n",
        "        train_data = batched(train, batch_size=64)\n",
        "        val_data = batched(test, batch_size=64)\n",
        "        model.fit(train_data, validation=val_data, verbose=True) ## Error occurs in training of the model\n",
        "\n",
        "        stratified = {}\n",
        "        for case in datagen.train_cases:\n",
        "            cancer_type = fetch.cancer_type(case)\n",
        "            cancer_type = fetch.disease_lookup[cancer_type]\n",
        "            if cancer_type is None: continue\n",
        "            stratified.setdefault(cancer_type, []).append(case)\n",
        "\n",
        "        with open(OUTFILE, \"w\") as outfile:\n",
        "            for cancer_type, cases in sorted(stratified.items()):\n",
        "                print (cancer_type, len(cases))\n",
        "                # if len(cases) < 64: continue\n",
        "                test_data = batched(datagen.data(mode='val', cases=cases), batch_size=64)\n",
        "\n",
        "                try:\n",
        "                    score = model.eval_data(test_data)\n",
        "                except:\n",
        "                    score = 0.0\n",
        "                print (f\"{cancer_type}, {score}\", file=outfile)\n",
        "\n",
        "    model.save(\"results/predict11.pth\") "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IK6TwR5hzVN",
        "colab_type": "text"
      },
      "source": [
        " **Not reproducible using the author's code**\n",
        "\n",
        "After the time we invested in rewriting the code to generate the data in the right format. And finding out their code for training the data still gives no result, we can conclude Table 2 is not reproducible using the code of the authors. And we only tried the modality combination of clinical and miRNA data for one cancer type yet. Imagine using all data modalities for every 20 cancer types. \n",
        "\n",
        "Working with the whole slide images wasn't even possible for us, because of the size.\n",
        "\n"
      ]
    }
  ]
}